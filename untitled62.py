# -*- coding: utf-8 -*-
"""Untitled62.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VPWUKtmaSDQ7wWBCGwZUuYMuwbR2TyJS
"""

import gradio as gr
import os
import re
import tempfile
import zipfile
import shutil
import pandas as pd
import fitz  # PyMuPDF
from collections import Counter

try:
    import pdfplumber
except ImportError:
    pdfplumber = None

class NCERTExtractor:
    def __init__(self):
        self.headers_blacklist = [
            "SHORT STORIES", "POETRY", "NON-FICTION", "DRAMA",
            "CONTENTS", "APPENDIX", "ANSWERS", "INDEX",
            "UNIT", "CHAPTER", "INTRODUCTION"
        ]
        self.garbage_prefixes = [
            "a short story is", "a poem is", "a drama is",
            "introduction to", "every poem that we read"
        ]

    def _get_metadata(self, filename):
        fn = filename.lower()
        meta = {
            "Board": "CBSE",
            "Class": "Unknown",
            "Subject": "Unknown",
            "Code": fn[:4]
        }

        if fn.startswith('l'): meta["Class"] = "XII"
        elif fn.startswith('k'): meta["Class"] = "XI"
        elif fn.startswith('j'): meta["Class"] = "X"
        elif fn.startswith('i'): meta["Class"] = "IX"
        elif fn.startswith('h'): meta["Class"] = "VIII"
        elif fn.startswith('g'): meta["Class"] = "VII"
        elif fn.startswith('f'): meta["Class"] = "VI"

        if any(x in fn for x in ['ph', 'ch', 'ma', 'mh', 'bi', 'sc']):
            if 'ph' in fn: meta["Subject"] = "Physics"
            elif 'ch' in fn: meta["Subject"] = "Chemistry"
            elif 'ma' in fn or 'mh' in fn: meta["Subject"] = "Mathematics"
            elif 'bi' in fn: meta["Subject"] = "Biology"
            elif 'sc' in fn: meta["Subject"] = "Science"
        elif any(x in fn for x in ['kl', 'fl', 'ev', 'eh', 'le']):
            if 'kl' in fn: meta["Subject"] = "English Elective"
            elif 'fl' in fn: meta["Subject"] = "English Core"
            else: meta["Subject"] = "English"
        elif 'hi' in fn: meta["Subject"] = "History"
        elif 'ge' in fn: meta["Subject"] = "Geography"

        return meta

    def _clean_text(self, text):
        if not text: return ""

        text = re.sub(r'not to be republished', '', text, flags=re.IGNORECASE)
        text = re.sub(r'Reprint\s+20\d{2}-?\d{2}', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\d{4,}CH\d{2}', '', text)
        text = re.sub(r'^\d+\s+', '', text)

        def merge_orphans(match):
            l, r = match.groups()
            if l.upper() in ['A', 'I']: return f"{l} {r}"
            return f"{l}{r}"
        text = re.sub(r'\b([A-Za-z])\s+([A-Za-z]{2,})\b', merge_orphans, text)

        def fix_spacing(m):
            return m.group(0).replace(" ", "")
        text = re.sub(r'\b([a-zA-Z]\s){3,}[a-zA-Z]\b', fix_spacing, text)


        words = text.split()
        seen = set()
        deduped = []
        for w in words:
            w_lower = w.lower()
            if w_lower not in seen:
                seen.add(w_lower)
                deduped.append(w)

        return " ".join(deduped).strip()

    def _get_running_headers(self, doc):
        counter = Counter()
        scan_limit = min(5, doc.page_count)
        for i in range(scan_limit):
            try:
                page = doc[i]
                header_area = page.get_text("blocks", clip=fitz.Rect(0, 0, page.rect.width, page.rect.height * 0.15))
                for b in header_area:
                    txt = b[4].strip()
                    if len(txt) > 3: counter[txt] += 1
            except: pass
        return {k for k, v in counter.items() if v >= (scan_limit / 2)}

    def _extract_toc(self, pdf_path):
        page_map = {}
        if not pdfplumber: return page_map
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for p in pdf.pages:
                    text = p.extract_text() or ""
                    for line in text.split('\n'):
                        match = re.search(r'^(.*?)\s+(\d+)$', line.strip())
                        if match:
                            t = match.group(1).strip()
                            p_num = int(match.group(2))
                            t = re.sub(r'^\d+[\.\s]+', '', t)
                            t = re.sub(r'\s+[A-Z\.\s]{3,}$', '', t)
                            page_map[p_num] = self._clean_text(t)
        except: pass
        return page_map

    def _get_printed_page_number(self, pdf_path):
        try:
            with pdfplumber.open(pdf_path) as pdf:
                p = pdf.pages[0]
                txt_b = p.crop((0, p.height * 0.9, p.width, p.height)).extract_text() or ""
                txt_t = p.crop((0, 0, p.width, p.height * 0.1)).extract_text() or ""
                full_txt = txt_b + " " + txt_t

                full_txt = re.sub(r'20\d{2}-?\d{2}', '', full_txt)
                full_txt = re.sub(r'\d{5,}', '', full_txt)

                nums = re.findall(r'\b\d{1,3}\b', full_txt)
                if nums: return int(nums[-1])
        except: pass
        return None

    def _extract_science_title(self, pdf_path):
        try:
            doc = fitz.open(pdf_path)
            if doc.page_count < 1: return "Unknown"
            page = doc[0]
            blocks = page.get_text("dict")["blocks"]
            doc.close()

            best_text = ""
            max_size = 0

            for b in blocks:
                if "lines" not in b: continue
                text = " ".join([" ".join([s["text"] for s in ln["spans"]]) for ln in b["lines"]]).strip()
                try:
                    size = max([s["size"] for ln in b["lines"] for s in ln["spans"]])
                except: size = 0

                if re.match(r'^(Chapter|Unit)\s*\d+', text, re.IGNORECASE): continue

                if size > max_size:
                    max_size = size
                    best_text = text

            return self._clean_text(best_text)
        except: return "Unknown"

    def _extract_arts_title(self, pdf_path):
        try:
            doc = fitz.open(pdf_path)
            if doc.page_count < 1: return "Unknown"

            blacklist = self._get_running_headers(doc)
            page = doc[0]
            blocks = page.get_text("dict")["blocks"]
            doc.close()

            candidates = []
            for b in blocks:
                if "lines" not in b: continue
                text = " ".join([" ".join([s["text"] for s in ln["spans"]]) for ln in b["lines"]]).strip()
                try:
                    size = max([s["size"] for ln in b["lines"] for s in ln["spans"]])
                except: size = 0

                if text in blacklist: continue
                if text.upper() in self.headers_blacklist: continue
                if any(text.lower().startswith(g) for g in self.garbage_prefixes): continue
                if re.match(r'^\d+$', text): continue

                candidates.append((size, text))

            candidates.sort(key=lambda x: x[0], reverse=True)

            for size, title in candidates:
                cleaned = self._clean_text(title)
                if len(cleaned) > 3: return cleaned

            return "Unknown"
        except: return "Unknown"

    def process_zip(self, zip_path):
        temp_dir = tempfile.mkdtemp()
        pdfs = []

        with zipfile.ZipFile(zip_path, 'r') as z:
            for f in z.namelist():
                if f.lower().endswith('.pdf') and not f.startswith('__MACOSX'):
                    z.extract(f, temp_dir)
                    pdfs.append(os.path.join(temp_dir, f))

        pdfs.sort()

        toc_map = {}
        for p in pdfs:
            if 'ps.pdf' in p.lower() or 'contents' in p.lower():
                toc_map = self._extract_toc(p)
                break

        results = []

        for p in pdfs:
            fname = os.path.basename(p)
            if fname.lower().endswith(('ps.pdf', 'an.pdf')): continue

            meta = self._get_metadata(fname)

            if meta["Subject"] in ["Physics", "Chemistry", "Mathematics", "Biology", "Science"]:
                title = self._extract_science_title(p)
            else:
                title = self._extract_arts_title(p)

            try:
                doc = fitz.open(p)
                page_count = doc.page_count
                doc.close()
            except: page_count = 0

            start_page = self._get_printed_page_number(p)

            if start_page and start_page in toc_map:
                title = toc_map[start_page]

            if start_page:
                end_page = start_page + page_count - 1
                page_range = f"{start_page}-{end_page}"
            else:
                page_range = f"1-{page_count} (Relative)"

            chap_match = re.search(r'(\d{2})\.pdf', fname)
            chapter = int(chap_match.group(1)) if chap_match else 0

            results.append({
                "Filename": fname,
                "Board": meta["Board"],
                "Class": meta["Class"],
                "Subject": meta["Subject"],
                "Chapter": chapter,
                "Title": title,
                "Page Range": page_range,
                "Total Pages": page_count
            })

        df = pd.DataFrame(results)
        cols = ["Filename", "Board", "Class", "Subject", "Chapter", "Title", "Page Range", "Total Pages"]
        df = df[cols] if set(cols).issubset(df.columns) else df

        csv_path = os.path.join(temp_dir, "Metadata.csv")
        json_path = os.path.join(temp_dir, "Metadata.json")
        df.to_csv(csv_path, index=False)
        df.to_json(json_path, orient='records', indent=2)

        return df, csv_path, json_path

def gradio_process(file_obj):
    if not file_obj:
        return None, None, None

    extractor = NCERTExtractor()

    if file_obj.name.endswith('.zip'):
        return extractor.process_zip(file_obj.name)
    else:
        temp_dir = tempfile.mkdtemp()
        dest = os.path.join(temp_dir, "upload.pdf")
        shutil.copy(file_obj.name, dest)
        zip_path = os.path.join(temp_dir, "upload.zip")
        with zipfile.ZipFile(zip_path, 'w') as z:
            z.write(dest, "upload.pdf")
        return extractor.process_zip(zip_path)

with gr.Blocks(title="NCERT Metadata Extractor") as app:
    with gr.Row():
        file_input = gr.File(label="Upload File", file_types=[".zip", ".pdf"])
        process_btn = gr.Button("Extract Metadata", variant="primary")

    with gr.Row():
        output_df = gr.Dataframe(label="Extracted Data")
        with gr.Column():
            output_csv = gr.File(label="Download CSV")
            output_json = gr.File(label="Download JSON")

    process_btn.click(
        fn=gradio_process,
        inputs=file_input,
        outputs=[output_df, output_csv, output_json]
    )

if __name__ == "__main__":
    app.launch()